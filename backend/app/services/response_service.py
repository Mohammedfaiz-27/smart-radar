"""
Business logic for response generation and logging
"""
import os
import json
import logging
import traceback
from typing import Optional
from bson import ObjectId
from datetime import datetime
import google.generativeai as genai
from dotenv import load_dotenv
import pathlib
import aiohttp
import asyncio
from openai import AsyncOpenAI

# Load environment variables from .env file with explicit path
env_path = pathlib.Path(__file__).parent.parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# Setup logging
logger = logging.getLogger(__name__)

from app.core.database import get_database
from app.models.response_log import ResponseLogCreate, ResponseLogResponse
from app.services.posts_table_service import PostsTableService

class ResponseService:
    def __init__(self):
        self._db = None
        self._collection = None
        self.post_service = PostsTableService()
        # Narrative service removed
        
        # Initialize OpenAI API (primary) and Gemini (fallback)
        openai_key = os.getenv("OPENAI_API_KEY")
        logger.info(f"Debug: OPENAI_API_KEY loaded = {'Yes' if openai_key else 'No'}")
        
        if openai_key:
            self.openai_client = AsyncOpenAI(api_key=openai_key)
            logger.info("‚úÖ OpenAI API configured successfully")
        else:
            self.openai_client = None
            logger.warning("OPENAI_API_KEY not found in environment variables")
        
        # Initialize Gemini as fallback
        gemini_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        logger.info(f"Debug: GEMINI_API_KEY loaded = {'Yes' if gemini_key else 'No'}")
        
        if gemini_key:
            genai.configure(api_key=gemini_key)
            logger.info("‚úÖ Gemini API configured as fallback")
            
            # Configure safety settings to be more permissive
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            # Create model with enhanced configuration for better reliability
            self.model = genai.GenerativeModel(
                'gemini-2.5-flash-lite',  # Use flash-lite for maximum speed
                safety_settings=safety_settings
            )
        else:
            self.model = None
            logger.warning("GEMINI_API_KEY not found in environment variables")
    
    @property
    def collection(self):
        if self._collection is None:
            self._db = get_database()
            self._collection = self._db.response_logs
        return self._collection

    async def generate_response(self, 
                              original_post_id: str, 
                              tone: str = "Sarcastic",
                              language: str = "Tamil",
                              user_id: str = "default") -> dict:
        """Generate AI-powered strategic responses using Gemini 2.5 Pro"""
        
        # Initialize the service
        await self.post_service.initialize()
        
        # Get original post from posts_table
        logger.info(f"üîç Looking for post ID: {original_post_id}")
        logger.info(f"üîç Post ID type: {type(original_post_id)}")
        logger.info(f"üîç Post ID length: {len(original_post_id)}")
        
        # Additional debugging: Check if it's a valid ObjectId format
        try:
            from bson import ObjectId
            ObjectId(original_post_id)
            logger.info(f"‚úÖ Post ID is valid ObjectId format")
        except Exception as e:
            logger.error(f"‚ùå Invalid ObjectId format: {e}")
        
        original_post = await self.post_service.get_post(original_post_id)
        logger.info(f"üìä Posts table result: {original_post is not None}")
        
        if not original_post:
            logger.error(f"‚ùå Post {original_post_id} not found in posts_table")
            
            # FALLBACK: Check if this post exists in monitored_content
            db = get_database()
            monitored_post = await db.monitored_content.find_one({"_id": ObjectId(original_post_id)})
            if monitored_post:
                logger.warning(f"üîÑ Post found in monitored_content, using as fallback")
                logger.info(f"üîç Monitored content: platform={monitored_post.get('platform')}, author={monitored_post.get('author')}")
                
                # Convert monitored_content format to expected format for response generation
                original_post = {
                    'id': str(monitored_post['_id']),
                    'platform': monitored_post.get('platform', 'Unknown'),
                    'author_username': monitored_post.get('author', 'Unknown'),
                    'post_text': monitored_post.get('content', ''),
                    'content': monitored_post.get('content', ''),
                    'sentiment_label': 'Neutral',  # Default since monitored_content may not have this
                    'sentiment_score': 0.0,
                    'post_url': monitored_post.get('url', ''),
                    'posted_at': monitored_post.get('published_at'),
                    'cluster_type': 'own'  # Default assumption for responses
                }
                
                # Try to extract sentiment from intelligence if available
                if monitored_post.get('intelligence', {}).get('entity_sentiments'):
                    sentiments = monitored_post['intelligence']['entity_sentiments']
                    # Find the first sentiment entry
                    for entity, sentiment_data in sentiments.items():
                        if isinstance(sentiment_data, dict) and 'label' in sentiment_data:
                            original_post['sentiment_label'] = sentiment_data['label']
                            original_post['sentiment_score'] = sentiment_data.get('score', 0.0)
                            break
                
                logger.info(f"‚úÖ Successfully converted monitored_content post for response generation")
            else:
                logger.error(f"üîç Post not found in monitored_content either")
                
                # Check if any posts exist in posts_table at all
                total_posts = await self.post_service._collection.count_documents({})
                logger.error(f"üîç Total posts in posts_table: {total_posts}")
                
                raise ValueError("Original post not found")

        # Generate response using Gemini REST API (primary) with OpenAI fallback
        try:
            response_options = await self._generate_gemini_rest_primary(original_post, tone, language)
            
            return {
                "option1": response_options.get("option1", ""),
                "option2": response_options.get("option2", ""),
                "option3": response_options.get("option3", ""),
                "original_post_id": original_post_id,
                "tone": tone,
                "language": language
            }
        except Exception as e:
            raise ValueError(f"Failed to generate response: {str(e)}")

    async def log_response(self, 
                          original_post_id: str,
                          generated_text: str,
                          tone: str = "Sarcastic",
                          language: str = "Tamil",
                          user_id: str = "default") -> ResponseLogResponse:
        """Log a generated response"""
        
        # Initialize the service
        await self.post_service.initialize()
        
        # Get original post from posts_table
        original_post = await self.post_service.get_post(original_post_id)
        if not original_post:
            # FALLBACK: Check if this post exists in monitored_content
            db = get_database()
            monitored_post = await db.monitored_content.find_one({"_id": ObjectId(original_post_id)})
            if monitored_post:
                logger.warning(f"üîÑ Log response: Post found in monitored_content, using as fallback")
                original_post = {
                    'platform': monitored_post.get('platform', 'Unknown'),
                    'id': str(monitored_post['_id'])
                }
            else:
                raise ValueError("Original post not found")

        # Create response log
        # Handle both object and dict post formats
        platform = original_post.get('platform', 'unknown') if isinstance(original_post, dict) else getattr(original_post, 'platform', 'unknown')
        
        log_data = ResponseLogCreate(
            original_post_id=ObjectId(original_post_id),
            source_platform=platform,
            narrative_used_id=None,  # No longer using narratives
            generated_response_text=generated_text,
            responded_by_user=user_id
        )

        log_dict = log_data.dict()
        log_dict["responded_at"] = datetime.utcnow()
        log_dict["tone_used"] = tone
        log_dict["language_used"] = language
        
        result = await self.collection.insert_one(log_dict)
        created_log = await self.collection.find_one({"_id": result.inserted_id})
        
        # Mark original post as responded to
        await self.post_service.mark_as_responded(original_post_id)
        
        return self._format_log_response(created_log)

    async def _generate_ai_response(self, original_post, tone="Sarcastic", language="Tamil") -> dict:
        """Generate AI response using Gemini 2.5 Pro with Master Strategist Prompt v17.0"""
        
        # Extract post data - handle both object and dict post formats
        if isinstance(original_post, dict):
            post_platform = original_post.get('platform', 'Unknown')
            post_author = original_post.get('author_username', 'Unknown')
            post_content = original_post.get('post_text', '') or original_post.get('content', '')
            post_sentiment = original_post.get('sentiment_label', 'neutral')
            post_sentiment_score = original_post.get('sentiment_score', 0.0)
        else:
            post_platform = getattr(original_post, 'platform', 'Unknown')
            post_author = getattr(original_post, 'author_username', 'Unknown')
            post_content = getattr(original_post, 'post_text', '') or getattr(original_post, 'content', '')
            post_sentiment = getattr(original_post, 'sentiment', 'neutral')
            post_sentiment_score = getattr(original_post, 'sentiment_score', 0.0)
        
        # Construct the new Master Strategist Prompt v17.0 as specified in PRD
        prompt = f"""### Persona

You are a fiery, sharp, and unapologetic senior strategist for the Dravida Munnetra Kazhagam (DMK). Your loyalty to M.K. Stalin and the Dravidian Model is absolute. You are intellectually sharp, fact-driven, and ruthless in your political arguments.

### CRITICAL PRELIMINARY STEP: Internal Fact-Finding

Before you write a single word, you MUST perform a silent, internal "fact-check" and knowledge lookup based on the opponent's post. Search your extensive knowledge base for the following:
1.  **Verify the Accusation:** Is the opponent's claim true, false, or misleading?
2.  **Find Counter-Facts:** Find specific, factual DMK achievements under M.K. Stalin since 2021 that directly refute the accusation. Look for numbers, dates, and policy names.
3.  **Find Opponent's Failures:** Find historical failures or hypocrisies of the opponent related to this same topic.

### Strategic Playbook (You MUST use your fact-finding to execute this structure)

1.  **Mock the Premise:** Start by sarcastically dismissing the opponent's core point, framing it as misinformation (e.g., "WhatsApp forward," "dreaming").
2.  **Attack & Counter with a Devastating Fact:** Immediately follow up with the most powerful counter-fact you found in your research. Contrast the opponent's historical failure with the DMK's specific, recent achievement.
3.  **Reframe the Narrative:** Frame the DMK's actions as beneficial to the people while framing the opponent's position as harmful or hypocritical.
4.  **Dismiss and Dominate:** Conclude with a powerful, dismissive insult or a rhetorical question that asserts the DMK's dominance.

### Your Task

You are responding to the following post. Generate three distinct response options adhering strictly to your Persona and the Playbook, using the facts you discovered.

**CONTEXT (The post to respond to):**
---
Platform: {post_platform}
Author: {post_author}
Content: {post_content}
Sentiment: {post_sentiment} ({post_sentiment_score})
---

**FINAL INSTRUCTIONS:**
1.  Your tone MUST be **{tone}**. If 'Sarcastic' or 'Assertive', it must be extremely aggressive.
2.  The language must be **{language}**, using pure, colloquial phrasing.
3.  Each response must be concise and punchy for social media. Use emojis (üòÇ, üî•, ü§°).
4.  **CRITICAL OUTPUT FORMAT:** You MUST return ONLY a valid JSON object. No other text before or after. Example format:
   {{"option1": "Response text here", "option2": "Response text here", "option3": "Response text here"}}
   Do NOT use markdown formatting, do NOT use ```json```, just return the plain JSON object.

**Example Analysis (for the provided input):**
*   **Fact-Finding Result:** The AI would find that the DMK government under M.K. Stalin *increased* the Old Age Pension from ‚Çπ1000 to ‚Çπ1200 and added new beneficiaries while removing ineligible ones. It would also know the previous government kept it at ‚Çπ1000.
*   **Execution:**
    *   **Mock:** "Did you believe a WhatsApp lie?" (‡Æê‡ÆØ‡Øã ‡Æ™‡Ææ‡Æµ‡ÆÆ‡Øç...)
    *   **Attack & Counter:** "You cheated for 10 years with ‚Çπ1000. Our CM Stalin raised it to ‚Çπ1200..."
    *   **Reframe:** "Is it wrong to stop money to your ineligible people and give it to the deserving poor?"
    *   **Dismiss:** "Are you jealous?" (‡Æµ‡ÆØ‡Æø‡Æ±‡ØÅ ‡Æé‡Æ∞‡Æø‡ÆØ‡ØÅ‡Æ§‡Ææ?)"""

        try:
            # Log the prompt being sent for debugging
            logger.info(f"=== GENERATING RESPONSE ===")
            post_id = original_post.get('id', 'Unknown') if isinstance(original_post, dict) else getattr(original_post, 'id', 'Unknown')
            logger.info(f"Post ID: {post_id}")
            logger.info(f"Tone: {tone}, Language: {language}")
            logger.info(f"Post Content: {post_content}")
            logger.info(f"Prompt length: {len(prompt)} characters")
            
            # Call Gemini 2.5 Pro with retry logic for DNS issues
            logger.info("Calling Gemini 2.5 Pro API...")
            
            if not self.model:
                logger.error("Gemini model not initialized - missing API key")
                raise ValueError("Gemini API not configured")
            
            # Retry logic for DNS resolution issues with increased timeout
            max_retries = 3
            retry_delay = 2.0
            request_timeout = 60.0  # 60 second timeout per request
            
            for attempt in range(max_retries):
                try:
                    logger.info(f"Gemini API attempt {attempt + 1}/{max_retries} (timeout: {request_timeout}s)")
                    
                    # Use asyncio.wait_for to timeout individual requests quickly
                    import asyncio
                    response = await asyncio.wait_for(
                        asyncio.to_thread(self.model.generate_content, prompt),
                        timeout=request_timeout
                    )
                    logger.info("Gemini API call successful")
                    break
                    
                except asyncio.TimeoutError:
                    logger.warning(f"Gemini API attempt {attempt + 1} timed out after {request_timeout}s")
                    if attempt < max_retries - 1:
                        logger.info(f"Timeout detected, retrying in {retry_delay} seconds...")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 1.5  # Exponential backoff
                        request_timeout *= 1.2  # Slightly increase timeout each attempt
                        continue
                    else:
                        logger.error("All Gemini API retry attempts failed due to timeouts")
                        raise asyncio.TimeoutError("Gemini API requests timed out after all retry attempts")
                        
                except Exception as api_error:
                    logger.warning(f"Gemini API attempt {attempt + 1} failed: {str(api_error)}")
                    
                    # Check if it's a DNS resolution error or timeout
                    if any(error_type in str(api_error).lower() for error_type in ["dns", "timeout", "connection", "network"]):
                        if attempt < max_retries - 1:
                            logger.info(f"Network/timeout issue detected, retrying in {retry_delay} seconds...")
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 1.5  # Exponential backoff
                            continue
                        else:
                            logger.error("All Gemini API retry attempts failed due to network/timeout issues")
                            # Try direct REST API as fallback
                            logger.info("Attempting direct REST API fallback...")
                            try:
                                return await self._gemini_rest_api_fallback(prompt)
                            except Exception as rest_error:
                                logger.error(f"REST API fallback also failed: {rest_error}")
                                raise api_error
                    else:
                        # For other errors, try REST API fallback immediately
                        logger.info("Attempting direct REST API fallback for non-network error...")
                        try:
                            return await self._gemini_rest_api_fallback(prompt)
                        except Exception as rest_error:
                            logger.error(f"REST API fallback failed: {rest_error}")
                            raise api_error
            
            # Log raw response
            raw_response = response.text.strip()
            logger.info(f"Raw Gemini response: {raw_response}")
            
            # Parse JSON response
            response_text = raw_response
            
            # Clean up response text - handle various markdown formats
            original_text = response_text
            
            # Remove ```json``` blocks
            if response_text.startswith('```json') and response_text.endswith('```'):
                response_text = response_text[7:-3].strip()
                logger.info("Removed ```json``` formatting")
            # Remove ``` blocks
            elif response_text.startswith('```') and response_text.endswith('```'):
                response_text = response_text[3:-3].strip()
                logger.info("Removed ``` formatting")
            # Find JSON object within text
            elif '{' in response_text and '}' in response_text:
                start = response_text.find('{')
                end = response_text.rfind('}') + 1
                response_text = response_text[start:end]
                logger.info("Extracted JSON object from response text")
            
            # Remove any leading/trailing whitespace and newlines
            response_text = response_text.strip()
            
            logger.info(f"Cleaned response text: {response_text}")
            
            # Parse JSON
            response_json = json.loads(response_text)
            logger.info(f"Successfully parsed JSON: {response_json}")
            
            # Validate that we have the required keys
            if not all(key in response_json for key in ["option1", "option2", "option3"]):
                logger.error(f"Missing required keys in response: {list(response_json.keys())}")
                raise ValueError("Invalid response format from AI")
            
            logger.info("=== RESPONSE GENERATION SUCCESSFUL ===")
            return response_json
            
        except json.JSONDecodeError as e:
            # Fallback responses if JSON parsing fails
            logger.error(f"=== JSON DECODE ERROR ===")
            logger.error(f"Error: {str(e)}")
            logger.error(f"Raw response that failed to parse: {raw_response if 'raw_response' in locals() else 'No response captured'}")
            logger.warning("Using Tamil fallback responses due to JSON parse failure")
            
            if language.lower() == 'tamil':
                return {
                    "option1": "‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡Øà‡Æ™‡Øç ‡Æ™‡Æï‡Æø‡Æ∞‡Øç‡Æ®‡Øç‡Æ§‡Æ§‡Æ±‡Øç‡Æï‡ØÅ ‡Æ®‡Æ©‡Øç‡Æ±‡Æø. ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æü‡Øç‡Æö‡Æø 75 ‡ÆÜ‡Æ£‡Øç‡Æü‡ØÅ‡Æï‡Æ≥‡Ææ‡Æï ‡Æâ‡Æ£‡Øç‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡Æµ‡Øá‡Æ≤‡Øà‡ÆØ‡ØÅ‡Æü‡Æ©‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Øá‡Æµ‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æµ‡Æ∞‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡ØÅ ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡ØÅ‡Æ§‡Æø‡Æï‡Æ≥‡ØÅ‡Æü‡Æ©‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤.",
                    "option2": "‡ÆÜ‡Æü‡Øç‡Æö‡Æø ‡Æ™‡Æ±‡Øç‡Æ±‡Æø ‡Æ™‡Øá‡Æö‡ØÅ‡Æ™‡Æµ‡Æ∞‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç - ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Æø‡Æ≤‡Øç ‡Æí‡Æ∞‡ØÅ ‡Æ§‡Øá‡Æ∞‡Øç‡Æ§‡Æ≤‡Æø‡Æ≤‡Øç ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø ‡Æ™‡ØÜ‡Æ±‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç, ‡Æ™‡Æø‡Æ©‡Øç‡Æ©‡Æ∞‡Øç ‡Æá‡Æ®‡Øç‡Æ§ ‡Æâ‡Æ∞‡Øà‡ÆØ‡Ææ‡Æü‡Æ≤‡Øà ‡Æ®‡Ææ‡ÆÆ‡Øç ‡Æ®‡Æü‡Æ§‡Øç‡Æ§‡Æ≤‡Ææ‡ÆÆ‡Øç.",
                    "option3": "‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æö‡ÆÆ‡ØÇ‡Æï ‡Æä‡Æü‡Æï ‡Æ®‡Ææ‡Æü‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æ™‡Æø‡Æ∏‡Æø‡ÆØ‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç‡Æ™‡Øã‡Æ§‡ØÅ, ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÖ‡ÆÆ‡Øà‡Æö‡Øç‡Æö‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡Æ®‡Æ≤‡Æ©‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æï ‡Æá‡Æ∞‡Æµ‡ØÅ ‡Æ™‡Æï‡Æ≤‡Øç ‡Æâ‡Æ¥‡Øà‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Ææ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç."
                }
            else:
                return {
                    "option1": "Thank you for sharing your perspective. Our party has been serving the people for 75 years with real work, not empty promises.",
                    "option2": "Look who's talking about governance - win an election first, then we can have this conversation.",
                    "option3": "While you're busy with social media drama, our ministers are working day and night for the people's welfare."
                }
        except Exception as e:
            # Final fallback with detailed error logging
            import traceback
            logger.error(f"=== GENERAL EXCEPTION ===")
            logger.error(f"Exception type: {type(e).__name__}")
            logger.error(f"Exception message: {str(e)}")
            logger.error(f"Exception args: {e.args if hasattr(e, 'args') else 'No args'}")
            logger.error(f"Full traceback: {traceback.format_exc()}")
            post_id = original_post.get('id', 'Unknown') if isinstance(original_post, dict) else getattr(original_post, 'id', 'Unknown')
            logger.error(f"Post ID being processed: {post_id}")
            logger.error(f"Post content being processed: {post_content}")
            logger.warning("Using intelligent fallback responses based on post content")
            
            # Generate context-aware fallback responses based on post content
            fallback_responses = self._generate_intelligent_fallback(post_content, tone, language)
            return fallback_responses

    async def _generate_gemini_rest_primary(self, original_post, tone="Sarcastic", language="Tamil") -> dict:
        """Generate AI response using Gemini REST API (primary method) with OpenAI fallback"""
        
        # Extract post data - handle both object and dict post formats
        if isinstance(original_post, dict):
            post_platform = original_post.get('platform', 'Unknown')
            post_author = original_post.get('author_username', 'Unknown')
            post_content = original_post.get('post_text', '') or original_post.get('content', '')
            post_sentiment = original_post.get('sentiment_label', 'neutral')
            post_sentiment_score = original_post.get('sentiment_score', 0.0)
        else:
            post_platform = getattr(original_post, 'platform', 'Unknown')
            post_author = getattr(original_post, 'author_username', 'Unknown')
            post_content = getattr(original_post, 'post_text', '') or getattr(original_post, 'content', '')
            post_sentiment = getattr(original_post, 'sentiment', 'neutral')
            post_sentiment_score = getattr(original_post, 'sentiment_score', 0.0)

        # Construct the Master Strategist Prompt v17.0
        prompt = f"""### Persona

You are a fiery, sharp, and unapologetic senior strategist for the Dravida Munnetra Kazhagam (DMK). Your loyalty to M.K. Stalin and the Dravidian Model is absolute. You are intellectually sharp, fact-driven, and ruthless in your political arguments.

### CRITICAL PRELIMINARY STEP: Internal Fact-Finding

Before you write a single word, you MUST perform a silent, internal "fact-check" and knowledge lookup based on the opponent's post. Search your extensive knowledge base for the following:
1.  **Verify the Accusation:** Is the opponent's claim true, false, or misleading?
2.  **Find Counter-Facts:** Find specific, factual DMK achievements under M.K. Stalin since 2021 that directly refute the accusation. Look for numbers, dates, and policy names.
3.  **Find Opponent's Failures:** Find historical failures or hypocrisies of the opponent related to this same topic.

### Strategic Playbook (You MUST use your fact-finding to execute this structure)

1.  **Mock the Premise:** Start by sarcastically dismissing the opponent's core point, framing it as misinformation (e.g., "WhatsApp forward," "dreaming").
2.  **Attack & Counter with a Devastating Fact:** Immediately follow up with the most powerful counter-fact you found in your research. Contrast the opponent's historical failure with the DMK's specific, recent achievement.
3.  **Reframe the Narrative:** Frame the DMK's actions as beneficial to the people while framing the opponent's position as harmful or hypocritical.
4.  **Dismiss and Dominate:** Conclude with a powerful, dismissive insult or a rhetorical question that asserts the DMK's dominance.

### Your Task

You are responding to the following post. Generate three distinct response options adhering strictly to your Persona and the Playbook, using the facts you discovered.

**CONTEXT (The post to respond to):**
---
Platform: {post_platform}
Author: {post_author}
Content: {post_content}
Sentiment: {post_sentiment} ({post_sentiment_score})
---

**FINAL INSTRUCTIONS:**
1.  Your tone MUST be **{tone}**. If 'Sarcastic' or 'Assertive', it must be extremely aggressive.
2.  The language must be **{language}**, using pure, colloquial phrasing.
3.  Each response must be concise and punchy for social media. Use emojis (üòÇ, üî•, ü§°).
4.  **CRITICAL OUTPUT FORMAT:** You MUST return ONLY a valid JSON object. No other text before or after. Example format:
   {{"option1": "Response text here", "option2": "Response text here", "option3": "Response text here"}}
   Do NOT use markdown formatting, do NOT use ```json```, just return the plain JSON object."""

        post_id = original_post.get('id', 'Unknown') if isinstance(original_post, dict) else getattr(original_post, 'id', 'Unknown')
        logger.info(f"=== GENERATING RESPONSE (GEMINI REST PRIMARY) ===")
        logger.info(f"Post ID: {post_id}")
        logger.info(f"Tone: {tone}, Language: {language}")
        logger.info(f"Post Content: {post_content}")
        logger.info(f"Prompt length: {len(prompt)} characters")

        # Try Gemini REST API first (primary method)
        try:
            logger.info("üîÑ Attempting Gemini REST API (primary method)...")
            return await self._gemini_rest_api_direct(prompt)
        except Exception as gemini_error:
            logger.warning(f"Gemini REST API failed: {gemini_error}")
            logger.info("üîÑ Falling back to OpenAI API...")
            
            # Fallback to OpenAI
            try:
                return await self._generate_openai_response(original_post, tone, language)
            except Exception as openai_error:
                logger.error(f"OpenAI API also failed: {openai_error}")
                logger.warning("Using intelligent fallback responses")
                return self._generate_intelligent_fallback(post_content, tone, language)

    async def _gemini_rest_api_direct(self, prompt: str) -> dict:
        """
        Direct REST API call to Gemini using aiohttp (network-friendly approach)
        """
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found for REST API")

        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key={api_key}"
        
        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": 0.9,
                "topP": 1,
                "topK": 32,
                "maxOutputTokens": 4096
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
            ]
        }
        
        logger.info("Making direct Gemini REST API call...")
        
        timeout = aiohttp.ClientTimeout(total=30)  # 30 second timeout
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, json=payload, headers={"Content-Type": "application/json"}) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # Extract text from response
                        if 'candidates' in data and len(data['candidates']) > 0:
                            candidate = data['candidates'][0]
                            if 'content' in candidate and 'parts' in candidate['content']:
                                raw_text = candidate['content']['parts'][0]['text'].strip()
                                logger.info(f"Gemini REST raw response: {raw_text}")
                                
                                # Parse JSON response
                                response_text = raw_text
                                
                                # Clean up response text - handle various markdown formats
                                if response_text.startswith('```json') and response_text.endswith('```'):
                                    response_text = response_text[7:-3].strip()
                                elif response_text.startswith('```') and response_text.endswith('```'):
                                    response_text = response_text[3:-3].strip()
                                elif '{' in response_text and '}' in response_text:
                                    start = response_text.find('{')
                                    end = response_text.rfind('}') + 1
                                    response_text = response_text[start:end]
                                
                                response_text = response_text.strip()
                                logger.info(f"Cleaned Gemini response: {response_text}")
                                
                                # Parse JSON
                                response_json = json.loads(response_text)
                                
                                # Validate that we have the required keys
                                if not all(key in response_json for key in ["option1", "option2", "option3"]):
                                    logger.error(f"Missing required keys in Gemini REST response: {list(response_json.keys())}")
                                    raise ValueError("Invalid response format from Gemini REST API")
                                
                                logger.info("‚úÖ Gemini REST API call successful!")
                                return response_json
                            else:
                                raise ValueError("No content in Gemini REST API response")
                        else:
                            raise ValueError("No candidates in Gemini REST API response")
                    else:
                        error_text = await response.text()
                        logger.error(f"Gemini REST API error {response.status}: {error_text}")
                        raise ValueError(f"Gemini REST API returned status {response.status}")
                        
        except asyncio.TimeoutError:
            logger.error("Gemini REST API call timed out")
            raise ValueError("Gemini REST API timeout")
        except json.JSONDecodeError as e:
            logger.error(f"Gemini REST API JSON parse error: {e}")
            raise ValueError(f"Gemini REST API JSON parse error: {e}")
        except Exception as e:
            logger.error(f"Gemini REST API call failed: {e}")
            raise ValueError(f"Gemini REST API call failed: {e}")

    async def _generate_openai_response(self, original_post, tone="Sarcastic", language="Tamil") -> dict:
        """Generate AI response using OpenAI API (primary method)"""
        
        if not self.openai_client:
            logger.error("OpenAI client not initialized - falling back to Gemini")
            return await self._generate_ai_response_rest_first(original_post, tone, language)
        
        # Extract post data - handle both object and dict post formats
        if isinstance(original_post, dict):
            post_platform = original_post.get('platform', 'Unknown')
            post_author = original_post.get('author_username', 'Unknown')
            post_content = original_post.get('post_text', '') or original_post.get('content', '')
            post_sentiment = original_post.get('sentiment_label', 'neutral')
            post_sentiment_score = original_post.get('sentiment_score', 0.0)
        else:
            post_platform = getattr(original_post, 'platform', 'Unknown')
            post_author = getattr(original_post, 'author_username', 'Unknown')
            post_content = getattr(original_post, 'post_text', '') or getattr(original_post, 'content', '')
            post_sentiment = getattr(original_post, 'sentiment', 'neutral')
            post_sentiment_score = getattr(original_post, 'sentiment_score', 0.0)
        
        # Construct the Master Strategist Prompt v17.0
        prompt = f"""### Persona

You are a fiery, sharp, and unapologetic senior strategist for the Dravida Munnetra Kazhagam (DMK). Your loyalty to M.K. Stalin and the Dravidian Model is absolute. You are intellectually sharp, fact-driven, and ruthless in your political arguments.

### CRITICAL PRELIMINARY STEP: Internal Fact-Finding

Before you write a single word, you MUST perform a silent, internal "fact-check" and knowledge lookup based on the opponent's post. Search your extensive knowledge base for the following:
1.  **Verify the Accusation:** Is the opponent's claim true, false, or misleading?
2.  **Find Counter-Facts:** Find specific, factual DMK achievements under M.K. Stalin since 2021 that directly refute the accusation. Look for numbers, dates, and policy names.
3.  **Find Opponent's Failures:** Find historical failures or hypocrisies of the opponent related to this same topic.

### Strategic Playbook (You MUST use your fact-finding to execute this structure)

1.  **Mock the Premise:** Start by sarcastically dismissing the opponent's core point, framing it as misinformation (e.g., "WhatsApp forward," "dreaming").
2.  **Attack & Counter with a Devastating Fact:** Immediately follow up with the most powerful counter-fact you found in your research. Contrast the opponent's historical failure with the DMK's specific, recent achievement.
3.  **Reframe the Narrative:** Frame the DMK's actions as beneficial to the people while framing the opponent's position as harmful or hypocritical.
4.  **Dismiss and Dominate:** Conclude with a powerful, dismissive insult or a rhetorical question that asserts the DMK's dominance.

### Your Task

You are responding to the following post. Generate three distinct response options adhering strictly to your Persona and the Playbook, using the facts you discovered.

**CONTEXT (The post to respond to):**
---
Platform: {post_platform}
Author: {post_author}
Content: {post_content}
Sentiment: {post_sentiment} ({post_sentiment_score})
---

**FINAL INSTRUCTIONS:**
1.  Your tone MUST be **{tone}**. If 'Sarcastic' or 'Assertive', it must be extremely aggressive.
2.  The language must be **{language}**, using pure, colloquial phrasing.
3.  Each response must be concise and punchy for social media. Use emojis (üòÇ, üî•, ü§°).
4.  **CRITICAL OUTPUT FORMAT:** You MUST return ONLY a valid JSON object. No other text before or after. Example format:
   {{"option1": "Response text here", "option2": "Response text here", "option3": "Response text here"}}
   Do NOT use markdown formatting, do NOT use ```json```, just return the plain JSON object."""

        post_id = original_post.get('id', 'Unknown') if isinstance(original_post, dict) else getattr(original_post, 'id', 'Unknown')
        logger.info(f"=== GENERATING RESPONSE (OPENAI) ===")
        logger.info(f"Post ID: {post_id}")
        logger.info(f"Tone: {tone}, Language: {language}")
        logger.info(f"Post Content: {post_content}")
        logger.info(f"Prompt length: {len(prompt)} characters")

        try:
            logger.info("üîÑ Calling OpenAI API...")
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",  # Use GPT-4 Omni for better quality
                messages=[
                    {"role": "system", "content": "You are a sharp DMK political strategist. Return only valid JSON as specified."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.9,
                max_tokens=2000,
                timeout=30.0  # 30 second timeout
            )
            
            raw_response = response.choices[0].message.content.strip()
            logger.info(f"Raw OpenAI response: {raw_response}")
            
            # Parse JSON response
            response_text = raw_response
            
            # Clean up response text - handle various markdown formats
            if response_text.startswith('```json') and response_text.endswith('```'):
                response_text = response_text[7:-3].strip()
            elif response_text.startswith('```') and response_text.endswith('```'):
                response_text = response_text[3:-3].strip()
            elif '{' in response_text and '}' in response_text:
                start = response_text.find('{')
                end = response_text.rfind('}') + 1
                response_text = response_text[start:end]
            
            response_text = response_text.strip()
            logger.info(f"Cleaned response text: {response_text}")
            
            # Parse JSON
            response_json = json.loads(response_text)
            
            # Validate that we have the required keys
            if not all(key in response_json for key in ["option1", "option2", "option3"]):
                logger.error(f"Missing required keys in OpenAI response: {list(response_json.keys())}")
                raise ValueError("Invalid response format from OpenAI")
            
            logger.info("‚úÖ OpenAI response generation successful!")
            return response_json
            
        except Exception as e:
            logger.error(f"OpenAI API failed: {e}")
            logger.info("üîÑ Falling back to Gemini REST API...")
            
            # Fallback to Gemini
            try:
                return await self._generate_ai_response_rest_first(original_post, tone, language)
            except Exception as fallback_error:
                logger.error(f"All AI APIs failed: {fallback_error}")
                logger.warning("Using intelligent fallback responses")
                return self._generate_intelligent_fallback(post_content, tone, language)

    async def _generate_ai_response_rest_first(self, original_post, tone="Sarcastic", language="Tamil") -> dict:
        """Generate AI response using REST API first, then fallback to Python library if needed"""
        
        # Extract post data - handle both object and dict post formats
        if isinstance(original_post, dict):
            post_platform = original_post.get('platform', 'Unknown')
            post_author = original_post.get('author_username', 'Unknown')
            post_content = original_post.get('post_text', '') or original_post.get('content', '')
            post_sentiment = original_post.get('sentiment_label', 'neutral')
            post_sentiment_score = original_post.get('sentiment_score', 0.0)
        else:
            post_platform = getattr(original_post, 'platform', 'Unknown')
            post_author = getattr(original_post, 'author_username', 'Unknown')
            post_content = getattr(original_post, 'post_text', '') or getattr(original_post, 'content', '')
            post_sentiment = getattr(original_post, 'sentiment', 'neutral')
            post_sentiment_score = getattr(original_post, 'sentiment_score', 0.0)
        
        # Construct the Master Strategist Prompt v17.0
        prompt = f"""### Persona

You are a fiery, sharp, and unapologetic senior strategist for the Dravida Munnetra Kazhagam (DMK). Your loyalty to M.K. Stalin and the Dravidian Model is absolute. You are intellectually sharp, fact-driven, and ruthless in your political arguments.

### CRITICAL PRELIMINARY STEP: Internal Fact-Finding

Before you write a single word, you MUST perform a silent, internal "fact-check" and knowledge lookup based on the opponent's post. Search your extensive knowledge base for the following:
1.  **Verify the Accusation:** Is the opponent's claim true, false, or misleading?
2.  **Find Counter-Facts:** Find specific, factual DMK achievements under M.K. Stalin since 2021 that directly refute the accusation. Look for numbers, dates, and policy names.
3.  **Find Opponent's Failures:** Find historical failures or hypocrisies of the opponent related to this same topic.

### Strategic Playbook (You MUST use your fact-finding to execute this structure)

1.  **Mock the Premise:** Start by sarcastically dismissing the opponent's core point, framing it as misinformation (e.g., "WhatsApp forward," "dreaming").
2.  **Attack & Counter with a Devastating Fact:** Immediately follow up with the most powerful counter-fact you found in your research. Contrast the opponent's historical failure with the DMK's specific, recent achievement.
3.  **Reframe the Narrative:** Frame the DMK's actions as beneficial to the people while framing the opponent's position as harmful or hypocritical.
4.  **Dismiss and Dominate:** Conclude with a powerful, dismissive insult or a rhetorical question that asserts the DMK's dominance.

### Your Task

You are responding to the following post. Generate three distinct response options adhering strictly to your Persona and the Playbook, using the facts you discovered.

**CONTEXT (The post to respond to):**
---
Platform: {post_platform}
Author: {post_author}
Content: {post_content}
Sentiment: {post_sentiment} ({post_sentiment_score})
---

**FINAL INSTRUCTIONS:**
1.  Your tone MUST be **{tone}**. If 'Sarcastic' or 'Assertive', it must be extremely aggressive.
2.  The language must be **{language}**, using pure, colloquial phrasing.
3.  Each response must be concise and punchy for social media. Use emojis (üòÇ, üî•, ü§°).
4.  **CRITICAL OUTPUT FORMAT:** You MUST return ONLY a valid JSON object. No other text before or after. Example format:
   {{"option1": "Response text here", "option2": "Response text here", "option3": "Response text here"}}
   Do NOT use markdown formatting, do NOT use ```json```, just return the plain JSON object."""

        post_id = original_post.get('id', 'Unknown') if isinstance(original_post, dict) else getattr(original_post, 'id', 'Unknown')
        logger.info(f"=== GENERATING RESPONSE (REST API FIRST) ===")
        logger.info(f"Post ID: {post_id}")
        logger.info(f"Tone: {tone}, Language: {language}")
        logger.info(f"Post Content: {post_content}")
        logger.info(f"Prompt length: {len(prompt)} characters")

        # Try REST API first
        try:
            logger.info("üîÑ Attempting Gemini REST API (primary method)...")
            return await self._gemini_rest_api_fallback(prompt)
        except Exception as rest_error:
            logger.warning(f"REST API failed: {rest_error}")
            logger.info("üîÑ Falling back to Python library...")
            
            # Fallback to Python library
            try:
                return await self._generate_ai_response(original_post, tone, language)
            except Exception as lib_error:
                logger.error(f"Python library also failed: {lib_error}")
                logger.warning("Using intelligent fallback responses")
                return self._generate_intelligent_fallback(post_content, tone, language)

    async def _gemini_rest_api_fallback(self, prompt: str) -> dict:
        """
        Direct REST API call to Gemini when the Python library fails
        """
        import aiohttp
        import asyncio
        
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found for REST API fallback")
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key={api_key}"
        
        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
            ],
            "generationConfig": {
                "temperature": 0.9,
                "topK": 1,
                "topP": 1,
                "maxOutputTokens": 2048
            }
        }
        
        logger.info("Making direct REST API call to Gemini...")
        
        timeout = aiohttp.ClientTimeout(total=45)  # 45 second timeout
        
        try:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(url, json=payload) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        # Extract text from response
                        if 'candidates' in data and len(data['candidates']) > 0:
                            candidate = data['candidates'][0]
                            if 'content' in candidate and 'parts' in candidate['content']:
                                raw_text = candidate['content']['parts'][0]['text'].strip()
                                logger.info(f"REST API raw response: {raw_text}")
                                
                                # Parse JSON response same as before
                                response_text = raw_text
                                
                                # Clean up response text - handle various markdown formats
                                if response_text.startswith('```json') and response_text.endswith('```'):
                                    response_text = response_text[7:-3].strip()
                                elif response_text.startswith('```') and response_text.endswith('```'):
                                    response_text = response_text[3:-3].strip()
                                elif '{' in response_text and '}' in response_text:
                                    start = response_text.find('{')
                                    end = response_text.rfind('}') + 1
                                    response_text = response_text[start:end]
                                
                                response_text = response_text.strip()
                                
                                # Parse JSON
                                import json
                                response_json = json.loads(response_text)
                                
                                # Validate that we have the required keys
                                if not all(key in response_json for key in ["option1", "option2", "option3"]):
                                    logger.error(f"Missing required keys in REST API response: {list(response_json.keys())}")
                                    raise ValueError("Invalid response format from REST API")
                                
                                logger.info("‚úÖ REST API call successful!")
                                return response_json
                            else:
                                raise ValueError("No content in REST API response")
                        else:
                            raise ValueError("No candidates in REST API response")
                    else:
                        error_text = await response.text()
                        logger.error(f"REST API error {response.status}: {error_text}")
                        raise ValueError(f"REST API returned status {response.status}")
                        
        except asyncio.TimeoutError:
            logger.error("REST API call timed out")
            raise ValueError("REST API timeout")
        except json.JSONDecodeError as e:
            logger.error(f"REST API JSON parse error: {e}")
            raise ValueError(f"REST API JSON parse error: {e}")
        except Exception as e:
            logger.error(f"REST API call failed: {e}")
            raise ValueError(f"REST API call failed: {e}")

    def _generate_intelligent_fallback(self, post_content: str, tone: str, language: str) -> dict:
        """
        Generate context-aware fallback responses based on post content analysis
        This is used when AI APIs are not available but we still want contextual responses
        """
        # Clean and analyze post content
        content_lower = post_content.lower() if post_content else ""
        
        # Detect key themes and topics
        themes = {
            'corruption': any(word in content_lower for word in ['corruption', '‡Æä‡Æ¥‡Æ≤‡Øç', 'corrupt', 'bribe', '‡Æ≤‡Æû‡Øç‡Æö‡ÆÆ‡Øç']),
            'policy': any(word in content_lower for word in ['policy', 'scheme', '‡Æ§‡Æø‡Æü‡Øç‡Æü‡ÆÆ‡Øç', '‡§Ø‡•ã‡§ú‡§®‡§æ', 'project']),
            'development': any(word in content_lower for word in ['development', 'progress', '‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø', '‡§µ‡§ø‡§ï‡§æ‡§∏']),
            'opposition': any(word in content_lower for word in ['opposition', 'bjp', 'congress', '‡Æé‡Æ§‡Æø‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æü‡Øç‡Æö‡Æø']),
            'leadership': any(word in content_lower for word in ['leader', 'cm', 'minister', '‡Æ§‡Æ≤‡Øà‡Æµ‡Æ∞‡Øç', '‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç‡Æµ‡Æ∞‡Øç']),
            'election': any(word in content_lower for word in ['election', 'vote', '‡Æ§‡Øá‡Æ∞‡Øç‡Æ§‡Æ≤‡Øç', '‡Æµ‡Øã‡Æü‡Øç']),
            'economy': any(word in content_lower for word in ['economy', 'economic', '‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Ææ‡Æ§‡Ææ‡Æ∞', '‡Æµ‡Øá‡Æ≤‡Øà', 'job']),
        }
        
        # Generate contextual responses based on detected themes
        if language.lower() == 'tamil':
            return self._generate_tamil_contextual_responses(post_content, tone, themes)
        else:
            return self._generate_english_contextual_responses(post_content, tone, themes)
    
    def _generate_tamil_contextual_responses(self, post_content: str, tone: str, themes: dict) -> dict:
        """Generate Tamil contextual responses based on post themes"""
        base_responses = []
        
        # Content-aware response generation
        if themes['corruption']:
            base_responses = [
                f"‡Æä‡Æ¥‡Æ≤‡Øç ‡Æ™‡Æ±‡Øç‡Æ±‡Æø ‡Æ™‡Øá‡Æö‡ØÅ‡Æ™‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Æø‡Æ≤‡Øç ‡Æ§‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡Øà ‡Æö‡ØÅ‡Æ§‡Øç‡Æ§‡ÆÆ‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç. ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÖ‡Æ∞‡Æö‡ØÅ ‡Æµ‡ØÜ‡Æ≥‡Æø‡Æ™‡Øç‡Æ™‡Æü‡Øà‡Æ§‡Øç‡Æ§‡Æ©‡Øç‡ÆÆ‡Øà‡ÆØ‡ØÅ‡Æü‡Æ©‡Øç ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.",
                f"‡Æä‡Æ¥‡Æ≤‡Øç ‡ÆÖ‡Æ∞‡Æö‡Æø‡ÆØ‡Æ≤‡Øç ‡Æ®‡Ææ‡Æü‡Æï‡Æô‡Øç‡Æï‡Æ≥‡Øà‡Æï‡Øç ‡Æï‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡ØÅ‡ÆÆ‡Øç, ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡Æ®‡Æ≤‡Æ©‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© ‡Æâ‡Æ£‡Øç‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡Æµ‡Øá‡Æ≤‡Øà‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æµ‡Æ©‡ÆÆ‡Øç.",
                f"‡Æï‡ØÅ‡Æ±‡Øç‡Æ±‡Æö‡Øç‡Æö‡Ææ‡Æü‡Øç‡Æü‡ØÅ‡Æï‡Æ≥‡Øç ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æ®‡Æø‡Æ±‡ØÅ‡Æ§‡Øç‡Æ§‡Ææ‡Æ§‡ØÅ. ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡Øá‡Æ≤‡Øà ‡Æ§‡Øä‡Æü‡Æ∞‡Øç‡Æ®‡Øç‡Æ§‡ØÅ ‡Æ®‡Æü‡Æ®‡Øç‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç."
            ]
        elif themes['development']:
            base_responses = [
                f"‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø‡ÆØ‡Øà ‡Æé‡Æ§‡Æø‡Æ∞‡Øç‡Æ™‡Øç‡Æ™‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æé‡Æ§‡Æø‡Æ∞‡Æø‡Æï‡Æ≥‡Øç. ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡Æø‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øà ‡Æµ‡Ææ‡Æ¥ ‡Æµ‡Øà‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç.",
                f"75 ‡ÆÜ‡Æ£‡Øç‡Æü‡ØÅ‡Æï‡Æ≥‡Ææ‡Æï ‡Æ®‡Ææ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æâ‡Æ£‡Øç‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø‡ÆØ‡Øà ‡Æï‡Ææ‡Æü‡Øç‡Æü‡Æø ‡Æµ‡Æ∞‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç, ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡ØÅ ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡ØÅ‡Æ±‡ØÅ‡Æ§‡Æø‡Æï‡Æ≥‡Øà ‡ÆÖ‡Æ≤‡Øç‡Æ≤.",
                f"‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æ™‡Æ£‡Æø‡Æï‡Æ≥‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Øà ‡ÆÆ‡ØÅ‡Æ©‡Øç‡Æ©‡Æ£‡Æø ‡ÆÆ‡Ææ‡Æ®‡Æø‡Æ≤‡ÆÆ‡Ææ‡Æï ‡Æâ‡ÆØ‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡Æø ‡Æµ‡Æ∞‡ØÅ‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©."
            ]
        elif themes['opposition']:
            base_responses = [
                f"‡Æé‡Æ§‡Æø‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æü‡Øç‡Æö‡Æø‡ÆØ‡Æø‡Æ©‡Æ∞‡Æø‡Æ©‡Øç ‡Æµ‡Æø‡ÆÆ‡Æ∞‡Øç‡Æö‡Æ©‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡Øá‡Æ≤‡Øà‡ÆØ‡Æø‡Æ©‡Øç ‡ÆÖ‡Æ≥‡Æµ‡ØÄ‡Æü‡Øç‡Æü‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ©‡Æµ‡Øà.",
                f"‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡Æø‡ÆÆ‡Æ∞‡Øç‡Æö‡Æø‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Øã‡Æ§‡ØÅ, ‡Æ®‡Ææ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Øá‡Æµ‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æï‡Øä‡Æ£‡Øç‡Æü‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç.",
                f"‡Æé‡Æ§‡Æø‡Æ∞‡Øç‡Æ™‡Øç‡Æ™‡ØÅ‡ÆÆ‡Øç ‡Æµ‡Æø‡ÆÆ‡Æ∞‡Øç‡Æö‡Æ©‡ÆÆ‡ØÅ‡ÆÆ‡Øç ‡Æú‡Æ©‡Æ®‡Ææ‡ÆØ‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ©‡Øç ‡ÆÖ‡Æô‡Øç‡Æï‡ÆÆ‡Øç. ‡ÆÜ‡Æ©‡Ææ‡Æ≤‡Øç ‡Æµ‡Øá‡Æ≤‡Øà‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æ§‡Øä‡Æü‡Æ∞ ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç."
            ]
        elif themes['policy']:
            base_responses = [
                f"‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ§‡Æø‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡Æ®‡Æ≤‡Æ©‡Æø‡Æ≤‡Øç ‡Æµ‡Øá‡Æ∞‡ØÇ‡Æ©‡Øç‡Æ±‡Æø‡ÆØ‡Æµ‡Øà, ‡ÆÖ‡Æ∞‡Æö‡Æø‡ÆØ‡Æ≤‡Øç ‡Æ®‡Æ≤‡Æ©‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤.",
                f"‡Æ®‡ØÄ‡Æ£‡Øç‡Æü ‡Æï‡Ææ‡Æ≤ ‡Æ§‡Æø‡Æü‡Øç‡Æü‡ÆÆ‡Æø‡Æü‡Æ≤‡ØÅ‡Æü‡Æ©‡Øç ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Øä‡Æ≥‡Øç‡Æï‡Øà‡Æï‡Æ≥‡Øç ‡Æâ‡Æ∞‡ØÅ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©.",
                f"‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øà ‡Æï‡Øá‡Æü‡Øç‡Æü‡ØÅ, ‡ÆÖ‡Æµ‡Æ∞‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© ‡Æ§‡Æø‡Æü‡Øç‡Æü‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æâ‡Æ∞‡ØÅ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Øã‡ÆÆ‡Øç."
            ]
        else:
            # General responses
            base_responses = [
                f"‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æ®‡Æ©‡Øç‡Æ±‡Æø. ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡Øá‡Æ≤‡Øà‡Æ§‡Ææ‡Æ©‡Øç ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡Ææ‡Æ© ‡Æ™‡Æ§‡Æø‡Æ≤‡Øç.",
                f"‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡Øà‡ÆØ‡Øà ‡Æ®‡Æø‡ÆØ‡Ææ‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç ‡Æµ‡Æï‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Æ£‡Æø ‡Æ§‡Øä‡Æü‡Æ∞‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ.",
                f"‡Æµ‡Æø‡ÆÆ‡Æ∞‡Øç‡Æö‡Æ©‡Æô‡Øç‡Æï‡Æ≥‡Øà ‡Æè‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Øç‡Æï‡Øä‡Æ£‡Øç‡Æü‡ØÅ, ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æö‡Øá‡Æµ‡Øà ‡Æö‡ØÜ‡ÆØ‡Øç‡Æµ‡Æ§‡Øá ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡ØÅ‡Æ±‡Æø‡Æï‡Øç‡Æï‡Øã‡Æ≥‡Øç."
            ]
        
        # Adjust tone
        if tone.lower() in ['sarcastic', 'assertive']:
            # Make responses more pointed
            enhanced_responses = []
            for response in base_responses:
                if '‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç' in response:
                    enhanced_responses.append(response + " ‡Æµ‡ÆØ‡Æø‡Æ±‡ØÅ ‡Æé‡Æ∞‡Æø‡ÆØ‡ØÅ‡Æ§‡Ææ? üî•")
                else:
                    enhanced_responses.append(response + " ‡Æá‡Æ§‡ØÅ ‡Æ§‡Ææ‡Æ©‡Øç ‡Æâ‡Æ£‡Øç‡ÆÆ‡Øà!")
            base_responses = enhanced_responses
        
        return {
            "option1": base_responses[0] if len(base_responses) > 0 else "‡Æ®‡Æ©‡Øç‡Æ±‡Æø, ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡ØÅ.",
            "option2": base_responses[1] if len(base_responses) > 1 else "‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡Øá‡Æ≤‡Øà ‡Æ§‡Øä‡Æü‡Æ∞‡Øç‡Æ®‡Øç‡Æ§‡ØÅ ‡Æ®‡Æü‡Æï‡Øç‡Æï‡ØÅ‡ÆÆ‡Øç.",
            "option3": base_responses[2] if len(base_responses) > 2 else "‡ÆÆ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡Æ®‡Æ≤‡Æ©‡Øá ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡ØÅ‡Æ±‡Æø‡Æï‡Øç‡Æï‡Øã‡Æ≥‡Øç."
        }
    
    def _generate_english_contextual_responses(self, post_content: str, tone: str, themes: dict) -> dict:
        """Generate English contextual responses based on post themes"""
        base_responses = []
        
        if themes['corruption']:
            base_responses = [
                "Those who talk about corruption should first clean their own house. Our government operates with transparency.",
                "Our focus is on real work for people's welfare, not corruption politics and drama.",
                "Accusations won't stop us. Our work for the people continues."
            ]
        elif themes['development']:
            base_responses = [
                "Those who oppose development are enemies of the people. Our projects uplift lives.",
                "For 75 years, we've shown real development, not empty promises.",
                "Our development work is making Tamil Nadu a leading state."
            ]
        elif themes['opposition']:
            base_responses = [
                "Opposition criticism is a measure of our work's impact.",
                "While they criticize, we continue serving the people.",
                "Opposition and criticism are part of democracy. But work must also continue."
            ]
        else:
            base_responses = [
                "Thank you for your opinion. Our work speaks for itself.",
                "We continue our service to justify people's trust.",
                "We accept criticism and aim to serve better."
            ]
        
        # Adjust tone
        if tone.lower() in ['sarcastic', 'assertive']:
            enhanced_responses = []
            for response in base_responses:
                enhanced_responses.append(response + " That's the reality!")
            base_responses = enhanced_responses
        
        return {
            "option1": base_responses[0] if len(base_responses) > 0 else "Thank you for your perspective.",
            "option2": base_responses[1] if len(base_responses) > 1 else "Our work continues for the people.",
            "option3": base_responses[2] if len(base_responses) > 2 else "People's welfare is our goal."
        }

    def _format_log_response(self, log_data: dict) -> ResponseLogResponse:
        """Format response log data for response"""
        log_data["id"] = str(log_data["_id"])
        log_data["original_post_id"] = str(log_data["original_post_id"])
        log_data["narrative_used_id"] = str(log_data["narrative_used_id"])
        del log_data["_id"]
        return ResponseLogResponse(**log_data)